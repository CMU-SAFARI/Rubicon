{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "902eae9a-69fb-4748-9750-76c319a5c0e5",
   "metadata": {},
   "source": [
    "## Skip Removal with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa308b35-7491-43b2-8d1b-f38b803ea865",
   "metadata": {},
   "source": [
    "SkipClip performs a gradual skip removal process with knowledge distillation (KD). KD is a model compression technique where a shallower model (student) learns to mimic a pre-trained bigger model (teacher) by transferring learned knowledge and label representation from the teacher to the student. SkipClip starts with a pre-trained over-parameterized model as the teacher, which is not updated during the training of the student network.  We achieve skip removal by letting the teacher teach the student to perform well on basecalling. At the start of every training epoch, SkipClip removes a skip connection from a block, starting from the input side, while performing KD. This is done until all skip connections are removed from the student network. SkipClip gets the best of both worlds: a highly accurate and topologically regular neural network without skip connections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9797ed-be91-40b2-9a89-eae2c04fb666",
   "metadata": {},
   "source": [
    "Download a model to use it as the teacher:\n",
    "``` bash\n",
    "$ cd models\n",
    "$ bash download_teacher.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3d10b70-420b-4672-b824-fdde9a98a64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No ROCm runtime is found, using ROCM_HOME='/opt/rocm-5.1.0'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from argparse import ArgumentDefaultsHelpFormatter\n",
    "from pathlib import Path\n",
    "from importlib import import_module\n",
    "import logging\n",
    "from rubicon.data import load_numpy_shuf,load_numpy_full\n",
    "from bonito.data import load_script\n",
    "from bonito.util import load_symbol, init,load_model\n",
    "from rubicon.util import __models__, default_config, default_data\n",
    "from rubicon.util import load_model_prune, load_model_prune_for_kd\n",
    "from rubicon.kdtraining import load_state, KDTrainer\n",
    "import toml\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from contextlib import redirect_stdout\n",
    "import subprocess\n",
    "import shutil\n",
    "from prettytable import PrettyTable\n",
    "import logging\n",
    "from rubicon.basemodule.prune import count_parameters\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1004ee2e-a4b9-4aab-814e-c68aba0bee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser\n",
    "from argparse import ArgumentDefaultsHelpFormatter\n",
    "from pathlib import Path\n",
    "from importlib import import_module\n",
    "import logging\n",
    "from rubicon.data import load_numpy_shuf,load_numpy_full\n",
    "from bonito.data import load_script\n",
    "from bonito.util import load_symbol, init,load_model\n",
    "from rubicon.util import __models__, default_config, default_data\n",
    "from rubicon.util import load_model_prune, load_model_prune_for_kd\n",
    "from rubicon.kdtraining import load_state, KDTrainer\n",
    "import toml\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from contextlib import redirect_stdout\n",
    "import subprocess\n",
    "import shutil\n",
    "from prettytable import PrettyTable\n",
    "import logging\n",
    "from rubicon.basemodule.prune import count_parameters\n",
    "_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1679889-9bdd-4f2b-9bfa-d75b70d732f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=1\n",
    "alpha=0.1\n",
    "structural=False\n",
    "l1_scoring=True\n",
    "prune_dim=0\n",
    "prune_proportion=0.1\n",
    "save_directory=\"skip_results\"\n",
    "workdir = os.path.expanduser(save_directory)\n",
    "onnx_name=\"\"\n",
    "onnx_name+=str(prune_proportion)\n",
    "force=True\n",
    "pretrained=\"\"\n",
    "teacher=True\n",
    "teacher_directory=\"../rubicon/models/bonito\"\n",
    "seed=25\n",
    "config=\"../rubicon/models/configs/config.toml\"\n",
    "device=\"cuda\"\n",
    "quant=False\n",
    "type=\"bonito\"\n",
    "full=False\n",
    "chunks=128\n",
    "valid_chunks=128\n",
    "batch=128\n",
    "restore_optim=False\n",
    "save_optim_every=10\n",
    "grad_accum_split=1\n",
    "epochs=10\n",
    "lr=2e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dec62b-ff2f-4162-a23c-c8a63342aa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 08:33:00 AM [INFO] Save path: skip_results\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 08:33:00] INFO (__main__/MainThread) Save path: skip_results\n",
      "[2023-11-07 08:33:00] INFO (__main__/MainThread) Save path: skip_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 08:33:00 AM [INFO] Loading Teacher Model:../rubicon/models/bonito\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 08:33:00] INFO (__main__/MainThread) Loading Teacher Model:../rubicon/models/bonito\n",
      "[2023-11-07 08:33:00] INFO (__main__/MainThread) Loading Teacher Model:../rubicon/models/bonito\n"
     ]
    }
   ],
   "source": [
    "temp=temp\n",
    "alpha=alpha\n",
    "workdir = os.path.expanduser(save_directory)   \n",
    "_logger.info(\"Save path: {}\".format(workdir))\n",
    "\n",
    "\n",
    "if force and os.path.exists(workdir) :\n",
    "    shutil.rmtree(workdir)\n",
    "if os.path.exists(workdir) and not force:\n",
    "    print(\"[error] %s exists, remove it to continue or use -f to force delete.\" % workdir)\n",
    "    exit(1)\n",
    "init(seed, device)\n",
    "device = torch.device(device)\n",
    "\n",
    "\n",
    "if pretrained:\n",
    "    dirname = pretrained\n",
    "    if not os.path.isdir(dirname) and os.path.isdir(os.path.join(__models__, dirname)):\n",
    "        dirname = os.path.join(__models__, dirname)\n",
    "    config_file = os.path.join(dirname, 'config.toml')\n",
    "else:\n",
    "    config_file = config\n",
    "config = toml.load(config_file)\n",
    "os.makedirs(workdir, exist_ok=True)    \n",
    "teacher_path=os.path.expanduser(teacher_directory)\n",
    "_logger.info(\"Loading Teacher Model:{}\".format(teacher_path))\n",
    "model_teacher = load_model(teacher_path, device, half=False)\n",
    "original_stdout = sys.stdout # Save a reference to the original standard output\n",
    "with open(workdir+'/model_student.txt', 'w') as f:\n",
    "        sys.stdout = f # Change the standard output to the file we created.       \n",
    "        if(pretrained):   \n",
    "                _logger.info(\"Using Pretrained Student:{}\".format(pretrained))\n",
    "                model_student = load_model_prune_for_kd(pretrained, device, half=False,load_model_type=stud_type, no_prune=no_prune)\n",
    "        else:              \n",
    "            if(type==\"rubiconqabas-mp\"):\n",
    "                _logger.info(\"Training a new student:{}\".format(type)) \n",
    "                model_student = load_symbol(config, 'RubiconSkipTrim')(config)  \n",
    "        count_parameters(model_student)\n",
    "      \n",
    "        if config.get(\"lr_scheduler\"):\n",
    "            sched_config = config[\"lr_scheduler\"]\n",
    "            lr_scheduler_fn = getattr(\n",
    "                import_module(sched_config[\"package\"]), sched_config[\"symbol\"]\n",
    "            )(**sched_config)\n",
    "        else:\n",
    "            lr_scheduler_fn = None\n",
    "        \n",
    "        _logger.info(\"Total parameters in model teacher:{}\".format(sum(p.numel() for p in model_teacher.parameters()))) \n",
    "        _logger.info(\"Total parameters in model student:{}\".format(sum(p.numel() for p in model_student.parameters())) )\n",
    "        _logger.info(\"Loading Data\")\n",
    "        if full:\n",
    "            _logger.info(\"Full dataset training\")\n",
    "            train_loader_kwargs, valid_loader_kwargs = load_numpy_full(None,\n",
    "                    directory\n",
    "            )\n",
    "        elif chunks:\n",
    "            _logger.info(\"Not full dataset training with shuffling\")\n",
    "            train_loader_kwargs, valid_loader_kwargs = load_numpy(\n",
    "                chunks,valid_chunks, directory\n",
    "            )\n",
    "        else:\n",
    "            _logger.warning(\"Please define the training data correctly\")\n",
    "            exit(1)\n",
    "\n",
    "        loader_kwargs = {\n",
    "            \"batch_size\": batch, \"num_workers\": 1, \"pin_memory\": True\n",
    "        }\n",
    "        train_loader = DataLoader(**loader_kwargs, **train_loader_kwargs)\n",
    "        valid_loader = DataLoader(**loader_kwargs, **valid_loader_kwargs)\n",
    "\n",
    "        _logger.info(\"Starting SkipTrim\")\n",
    "\n",
    "        trainer = KDTrainer(\n",
    "            model_teacher,model_student, device, train_loader, valid_loader,\n",
    "            use_amp=False,\n",
    "            lr_scheduler_fn=lr_scheduler_fn,\n",
    "            restore_optim=restore_optim,\n",
    "            save_optim_every=save_optim_every,\n",
    "            grad_accum_split=grad_accum_split,\n",
    "            temp=temp,\n",
    "            alpha=alpha\n",
    "        )\n",
    "\n",
    "        trainer.fit(workdir, epochs, lr, skip_stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acded46-4a9d-45e7-98b7-c75b7f6ed1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
